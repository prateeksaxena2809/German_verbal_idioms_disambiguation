{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tjz3FDFNz2Y",
        "outputId": "8d32abb9-ba58-452c-f68e-2d0cd012c314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 7.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT_etjfG3zyY",
        "outputId": "6c475413-e96c-4053-bb3a-ad4d69a26dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNVZNQK6OMQG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification,XLMRobertaTokenizer,XLMRobertaForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "label_dict={\"literally\":0,\"figuratively\":1,\"both\":2,\"undecidable\":3}\n",
        "\n",
        "def create_sentences_tags_labels(filename):\n",
        "  _,_,labels,sentences=zip(*[line.split(\"\\t\") for line in open(filename).readlines()])\n",
        "  print(len(open(filename).readlines()))\n",
        "\n",
        "  labels=list(map(lambda x:label_dict[x.strip()],labels))\n",
        "  tags_list=[]\n",
        "  sentence_list=[]\n",
        "  for sentence in sentences:\n",
        "    words=sentence.strip().split()\n",
        "    b_words=[i for i,word in enumerate(words) if word.startswith(\"<b>\") and word.endswith(\"</b>\")]\n",
        "    tags=[\"O\" for _ in range(len(words))]\n",
        "    tags=tags[:b_words[0]]+[\"B\" for i in range(b_words[0],b_words[-1]+1)]+tags[b_words[-1]+1:]\n",
        "    tags_list.append(tags)\n",
        "    sentence_list.append(\" \".join(words).replace(\"<b>\",\"\").replace(\"</b>\",\"\"))\n",
        "  print(len(sentence_list),len(tags_list),len(labels))\n",
        "  return sentence_list,tags_list,labels\n",
        "\n",
        "\n",
        "#\"drive/MyDrive/vid-disambiguation-sharedtask-main/data/dev/dev.tsv\")\n",
        "test_sentences,test_tags,test_labels=create_sentences_tags_labels(\"test.tsv\")\n",
        "train_sentences,train_tags,train_labels=create_sentences_tags_labels(\"train.tsv\")\n",
        "\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
        "\n",
        "def idiom_tokenize(tokenizer, sentences, tags_list):\n",
        "  tokenized_sentences=tokenizer(sentences,padding=True,truncation=True,max_length=128)\n",
        "  print(tokenized_sentences.keys())\n",
        "  input_ids_list=tokenized_sentences[\"input_ids\"]\n",
        "  \n",
        "  for i, (tags, input_ids, sentence) in enumerate(zip(tags_list, input_ids_list, sentences)):\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    tokens = tokens[1:-1]\n",
        "    new_ids=[]\n",
        "    for tag, word in zip(tags, sentence.strip().split()):\n",
        "      word=word.lower()\n",
        "      while word != \"\" and len(tokens)>0:\n",
        "        tokens[0] = tokens[0].replace(\"##\",\"\")\n",
        "        word=word[len(tokens[0]):]\n",
        "        new_ids.append(0 if tag==\"O\" else 1)\n",
        "        tokens=tokens[1:]\n",
        "      if len(tokens)==0:\n",
        "        break\n",
        "    tokenized_sentences[\"token_type_ids\"][i]=[0]+new_ids+[0]+[0]*len(tokens)\n",
        "  for i,j in zip(tokenized_sentences[\"input_ids\"],tokenized_sentences[\"token_type_ids\"]):\n",
        "    assert len(i)==len(j)\n",
        "  return tokenized_sentences\n",
        "\n",
        "class IdiomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChvcoyJa0Mkc",
        "outputId": "b7234ac2-856b-47dc-a920-bc3ad50090fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/dbmdz/bert-base-german-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dd6b21985a95123a88e7444ca271cc9429cf8c4ccbf84e5665ffebb3db0958ca.6ec690b98e01c56d26601258d2be34c3e5a76b949465ed58983cff81e5f9fa88\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dbmdz/bert-base-german-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/dbmdz/bert-base-german-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4202b2a351b48cfcd7a682c7c364388db03d3b02f90f8036bea4fd062b361be1.d2b0963a208cae5aef8c0ffa9604e6d54304034f96022a072ba98a05e4514d88\n",
            "Some weights of the model checkpoint at dbmdz/bert-base-german-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences_train=idiom_tokenize(tokenizer,train_sentences,train_tags)\n",
        "tokenized_sentences_test=idiom_tokenize(tokenizer,test_sentences,test_tags)\n",
        "\n",
        "train_dataset=IdiomDataset(tokenized_sentences_train,train_labels)\n",
        "test_dataset=IdiomDataset(tokenized_sentences_test,test_labels)\n",
        "\n",
        "model=AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-german-uncased\",num_labels=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTEQJ4RCbsqF",
        "outputId": "a528d95d-64fd-48f9-9fad-fd5c546009fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall_0,recall_1,recall_2,recall_3 = recall_score(y_true=labels, y_pred=pred,average=None)\n",
        "    precision_0,precision_1,precision_2,precision_3 = precision_score(y_true=labels, y_pred=pred,average=None)\n",
        "    f1_0,f1_1,f1_2,f1_3 = f1_score(y_true=labels, y_pred=pred,average=None)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \n",
        "            \"precision_0\": precision_0,\n",
        "            \"precision_1\": precision_1,\n",
        "            \"recall_0\": recall_0,\n",
        "            \"recall_1\": recall_1,\n",
        "            \"f1_0\": f1_0,\n",
        "            \"f1_1\": f1_1}\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    seed=0,\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit = 1,\n",
        "    learning_rate=1e-05,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-3DpP4Onb-fT",
        "outputId": "5d5ffa2f-e25c-4785-ff82-fb84b9d55c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 6902\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 4315\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4315' max='4315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4315/4315 11:55, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision 0</th>\n",
              "      <th>Precision 1</th>\n",
              "      <th>Recall 0</th>\n",
              "      <th>Recall 1</th>\n",
              "      <th>F1 0</th>\n",
              "      <th>F1 1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.384000</td>\n",
              "      <td>0.291977</td>\n",
              "      <td>0.921906</td>\n",
              "      <td>0.851163</td>\n",
              "      <td>0.933642</td>\n",
              "      <td>0.690566</td>\n",
              "      <td>0.977383</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.955012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.173200</td>\n",
              "      <td>0.261098</td>\n",
              "      <td>0.931833</td>\n",
              "      <td>0.794326</td>\n",
              "      <td>0.963385</td>\n",
              "      <td>0.845283</td>\n",
              "      <td>0.956381</td>\n",
              "      <td>0.819013</td>\n",
              "      <td>0.959870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.109800</td>\n",
              "      <td>0.361276</td>\n",
              "      <td>0.936466</td>\n",
              "      <td>0.826415</td>\n",
              "      <td>0.959872</td>\n",
              "      <td>0.826415</td>\n",
              "      <td>0.966074</td>\n",
              "      <td>0.826415</td>\n",
              "      <td>0.962963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.075100</td>\n",
              "      <td>0.395383</td>\n",
              "      <td>0.936466</td>\n",
              "      <td>0.823970</td>\n",
              "      <td>0.960611</td>\n",
              "      <td>0.830189</td>\n",
              "      <td>0.965267</td>\n",
              "      <td>0.827068</td>\n",
              "      <td>0.962933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.056600</td>\n",
              "      <td>0.404363</td>\n",
              "      <td>0.936466</td>\n",
              "      <td>0.819188</td>\n",
              "      <td>0.962097</td>\n",
              "      <td>0.837736</td>\n",
              "      <td>0.963651</td>\n",
              "      <td>0.828358</td>\n",
              "      <td>0.962873</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1511\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-863\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-863/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-863/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-4315] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1511\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-1726\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-1726/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-1726/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-863] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1511\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-2589\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-2589/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-2589/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1511\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-3452\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-3452/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-3452/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-2589] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1511\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-4315\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-4315/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-4315/pytorch_model.bin\n",
            "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-3452] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/Colab Notebooks/prateek_models/output/checkpoint-1726 (score: 0.26109760999679565).\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_state()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "run_classification.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}